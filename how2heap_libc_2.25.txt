Heap Exploitation glibc version 2.25 it is based on how2heap + a

Have to list : Fix detail about main_arena + xx, make PoC source code and binaries

0. heap
	1. Chunk 
		a. Allocated chunk : [prev_size or Chunk] | Size of chunk | Data | Size of next chunk
		b. Free chunk : [Chunk or prev_size] | Chunk size | [fd] | bk | [fd_nextsize] | [bk_nextsize] | Unused Space | Size of next chunk ( main_arena <bk---fd> chunk1 <bk---fd> chunk2 <bk---fd> main_arena )
		c. Top chunk : the last in the memory, resized when malloc asks for more space from the os. If the requested size is smaller than 0x21000, extending the top chunk to adding page size, if not, mmap a new page
	2. Bin
		a. Fast bin : P.fd == next_chunk.prev_size, 0x10~0x58 bytes(0x20~0xb0 bytes in x64), 10 bins(single linked list, LIFO), Free chunk Merge X
		b. Small bin : 0x10~0x1f8(0x20~0x3f0 in x64) bytes, 62 bins(double linked list, FIFO), Free chunk Merge
		c. Large bin : 0x200~0x3f0(0x400~0x7e0 in x64) bytes, 63 bins(2 double linked list), Free chunk Merge, The fd_nextsize in the large bin chunk points to the first chunk in the list that is smaller than itself, and bk_nextsize points to the first chunk larger than itself.
		d. Unsorted bin : Freed Small, Large chunk, 1 bin(double linked list)
	3. malloc
		a. Memory aligned : chunk.size is always positioned at 0x~~~8 and chunk.prev_size is always positioned at 0x~~~0 (in x64), so if we request 0x~~0 bytes, 8 bytes are more allocated for prev_size for prev_size, chunk.size = requested(chunk) + 0x8(chunk.size) + 0x8, usable(chunk) = requested(chunk) + 0x8
	4. free : FastChunkSize?FastBin:UnsrotedBin, FastBin,UnsortedBin=>SmallSize?SmallBin:LargeBin
		a. unlink : prev_inuse?nop:consolidate( P→fd->bk = P→bk; P→bk->fd = P->fd ), bin[x] => [a] <bk---fd> [b] <bk---fd> [c] in this situation consolidated [b+d] must move to bin[x+i], so do unlink() 

		
1. House of Force - Overwrite Top Chunk
	1. Conditions
		a.  1 malloc(size) #A( Writable, overflow to top_chunk.size )
		b.  1 malloc(evil_size) #B( We have to control size ) 
		c.  1 malloc(Size) #C( Writable )
	2. Exploit
		a. malloc(size) #A : set up memory
		b. *(A + usable(A)) = 0xffffffffffffffff : top_chunk.size = 0xffffffffffffffff, Now, malloc never call mmap
		c. malloc(evil_size) #B : evil_size = target - old_top - 2*2*SIZE_SZ. Now, new_top = old_top + 2*SIZE_SZ + evil_size, == target - 2*SIZE_SZ
		e. malloc(Size) #C : target is allocated
	3. Value
		a. Manipulate data at semi-arbitary memory (higher address than top_chunk)


2. House of Spirit - Fake Chunk to Fast Bin
	1. Conditions
		a. 2 stack buffer : #A <---we can overwrite here, fast chunk size---> #B
		b. 1 malloc(ANY) : set up memory
		c. 1 free(A)
		d. 1 malloc(usable(B - A) - 2*SIZE_SZ)
	2. Exploit
		a. malloc(ANY) : set up memory
		b. fake_chunk1.size = fake_chunk2 - fake_chunk1 : fake_chunk1 + size == fake_chunk2
		c. fake_chunk2.size = Size :  2*SIZE_SZ < Size < av->system_mem
		d. free(fake_chunk1 + 2*SIZE_SZ) : FastBin => fake_chunk1
		e. malloc(requested(fake_chunk1)) : fake_chunk1 is allocated
	3. Value
		a. Make buffer overflow attack vector
		
		
3. House of Lore - Bypass Small Bin Corrupted( bck->fd != victim )
	1. Conditions
		a. 2 stack buffer #fake_chunk1( writable, size >= 4*SIZE_SZ ), #fake_chunk2( writable, size >= SIZE_SZ )
		b. 3 malloc(small_bin_size) #A( writable after free ), #B, #C( writable )
		c. 1 malloc(large_bin_size)
		d. 1 free(A)
	2. Exploit
		a. malloc(small_chunk_size) #A
		b. fake_chunk1.prev_size = 0, fake_chunk1.size = 0, fake_chunk1.fd = A - 2*SIZE_SZ, fake_chunk1.bk = fake_chunk2, fake_chunk2.fd = fake_chunk1 : fake_chunk2 <bk---fd> fake_chunk1 <---fd> A
		c. malloc(large_chunk_size) : prevent consolidating the top chunk with A
		d. free(A) : UnsortedBin => A
		e. malloc(large_chunk_size) : SmallBin => fake_chunk2 <bk---fd> fake_chunk1 <---fd> A <bk---fd> main_arena
		f. A.bk = fake_chunk1 : SmallBin => fake_chunk2 <bk---fd> fake_chunk1 <bk---fd> A <bk---fd> main_arena
		g. malloc(requested(A)) #B : A is allocated, SmallBin => fake_chunk2 <bk---fd> fake_chunk1 <bk---fd> main_arena
		h. malloc(requested(A)) #C : fake_chunk1 is allocated, SmallBin => fake_chunk2 <bk---fd> main_arena

	3. Value
		a. Make buffer overflow attack vector
		
		
4. first fit - UAF
	1. Conditions
		a. 2 malloc(size) #A( writable after free), #B( non-writable )
		b. 1 free(A) 
	2. Exploit
		a. malloc(size) #A
		a. free(A)
		b. malloc(size) #B : A == B, we can control B via A
	3. Value
		a. Manipulate heap data via duplicate heap pointer
		

5. fastbin dup - DFB
	1. Conditions
		a. 3 malloc(fast_chunk_size) #A, #B, #C 
		b. free(A), free(B), free(A)
		c. 3 malloc(fast_chunk_size)
	2. Exploit
		a. 3 malloc(fast_chunk_size) #A, #B, #C : C for preventing consolidating top chunk with B
		b. free(A) : FastBin => A ---fd> main_arena
		c. free(B) : FastBin => B ---fd> A ---fd> main_arena 
		d. free(A) : FastBin => A ---fd> B ---fd> A ---fd> B ...
		e. 3 malloc(requested(A)) : A, B, A are allocated. A is duplicated
	3. Value
		a. Manipulate heap data via Duplicate heap pointer
		
		
6. fastbin dup into stack
	1. Conditions
		a. 1 stack buffer #fake_chunk( size >= SIZE_SZ )
		b. 3 malloc(fast_chunk_size) #A, #B, #C
		c. free(A), free(B), free(A)
		d. 2 malloc(fast_chunk_size) #D( writable ), #E 
		e. 2 malloc(fast_chunk_size)
	2. Exploit
		a. 3 malloc(fast_chunk_size) #A, #B, #C : C for preventing consolidating top chunk with B
		b. free(A) -> free(B) -> free(A) : build up fastbin_dup
		c. 2 malloc(requested(A)) #D, #E : A and B is allocated, FastBin => A ---fd> B ---fd> A ...
		d. fake_chunk.size = A.size, D.fd = fake_chunk : FastBin A ---fd> fake_chunk ---x>
		e. 2 malloc(requested(A)) : A and fake_chunk is allocated
	3. Value
		a. Make buffer overflow attack vector


7. House of einherjar - off by one
	1. Conditions
		a. 1 buffer #fake_chunk( size >= 6*SIZE_SZ )
		b. 2 malloc(size) #A( writable, off by one to B.size.prev_inuse ), #B
		c. free(B)
		d. malloc(Size)
	2. Exploit
		a. 2 malloc(size) #A, #B : We can use off by one if B.size is multiple of 0x100
		b. fake_chunk.prev_size = ANY, fake_chunk.size = ANY, fake_chunk.fd = fake_chunk, fake_chunk.bk = fake_chunk, fake_chunk.fd_nextsize = fake_chunk, fake_chunk.bk_nextsize = fake_chunk
		c. *(A + usable(A) - SIZE_SZ) = (B - 2*SIZE_SZ) - fake_chunk, *(A + usable(A)) = 0 : B.prev_size = -(B to fake_chunk), B.size = B.size | 0xff00, via off by one
		d. fake_chunk.size = B.prev_size : bypass "unlink : corrupted size vs. prev_size"
		e. free(B) : 
			1. If B is last chunk( malloc(C) before free(B) ) : fake_chunk & B & top_chunk are consolidated, unlink fake_chunk, new_top == fake_chunk
				a. malloc(Size) : fake_chunk is allocated
			2. If B is not last chunk : fake_chunk & B are consolidated, unlink fake_chunk, UnsortedBin => fake_chunk
				a. fake_chunk.size = SIZE, 2*SIZE_SZ <= SIZE <= av->system_mem : bypass " malloc() : memory corruption"
				b. malloc(smaller_than_fake_chunk_size) : fake_chunk is allocated
	3. Value
		a. Make buffer overflow attack vector
		
		
8. Poison null byte - off by one
	1. Conditions
		a. 4 malloc(larger_than_fast_chunk) #A( writable, off by one to B.size ), #B( writable ), #C, #D
		b. free
	2. Exploit
		a. 4 malloc(larger_than_fast_chunk) #A, #B, #C, #D : D for preventing consolidating top chunk with C
		b. free(B) : UnsortedBin => B, C.prev_size == B.size, C.size.prev_inuse == 0
		d. *(A + usable(A)) = 0x00 : B.size = B.size|0xff00
		c. *(B + B.size - 2*SIZE_SZ) = B.size|0xff00 : fake_C_prevsize = B.size|0xff00, fake_C_prevsize == B.size, bypass "unlink : corrupted size vs. prev_size"
		e. malloc(smaller_than_B) #B1 : C.prev_size is not changed, B is allocated, unlink B, (B.size - B1.size)UnsortedBin => remain_B
		f. malloc(smaller_than_B1) #B2 : C.prev_size is not changed,(B + B1.size) is allocated, unlink remain_B, (B.size - B1.size - B2.size)UnsortedBin => remain2_B
		g. free(B1) : (B1.size)UnsortedBin => B2
		h. free(C) : C.prev_chunk == C - C.prev_size == B1, B1 and C are consolidated, unlink B1, (C.prev_size + C.size)UnsortedBin => B1
		i. malloc(C.prev_size + C.size - SIZE_SZ) : C.prev_size + C.size is already memory aligned. B1 is allocated, overlapping B2
	3. Value
		a. Manipulate heap data via overlapping


9. Overlapping chunks
	1. Conditions
		a.  3 malloc(larger_than_fast_chunk) #A( writable, BOF to B.size ), #B, #C 
		b.  free(B) 
		c.  malloc(B.size + C.size - SIZE_SZ) 
	2. Exploit 
		a. 3 malloc(larger_than_fast_chunk) #A, #B, #C : C for preventing consolidating top chunk with B
		b. free(B) : UnsortedBin => B
		c. B.size = (B.size + C.size) | 0x1
		d. malloc(B.size + C.size - SIZE_SZ) : B.size and C.size is already aligned, so in caculate process, we don't consider about memory aligned. B + C is allocated
	3. Value
		a. Manipulate heap data via overlapping


10. Overlapping chunks 2
	1. Conditions
		a. [ 5 malloc() #A( writable, BOF to B.size ), #B, #C, #D, #E ]
		b. [ free(D) ]
		c. [ malloc(Size) ]
	2. Exploit
		a. 5 malloc(larger_than_fast_chunk) #A, #B, #C, #D, #E : E for preventing consolidating top chunk with D
		b. free(D) : UnsortedBin => D
		c. B.size = (B.size + C.size) | 0x1 : B.next_chunk => D
		d. free(B) : D.prev_size = B.size + C.size, B & D are consolidated, unlink B1, UnsortedBin => B
		e. malloc(B.size + C.size + D.size) : B + C + D is allocated
	3. Value
		a. Manipulate heap data via overlapping
		
		
11. unsorted bin attack
	1. Conditions
		a. 2 malloc(larger_than_fast_chunk) #A( writable after free ), #B 
		b. free(A) 
		c. malloc((A.size))
	2. Exploit 
		a. 2 malooc(larger_than_fast_chunk) #A, #B : B for preventing consolidating top chunk with A
		b. free(A) : UnsortedBin => main_arena+88 <bk---fd> A <bk---fd> main_arena+88
		c. A.bk = target_address - 2*SIZE_SZ : UnsortedBin => target_address <bk---> A <bk---fd> main_arena+88
		d. malloc(requested_size_A) : unlink A, target_address <bk---fd> main_arena+88, *(target_address) = main_arena+88
	3. Value
		a. Bypass null check or Setting attack contidion related to main_arena or overflow
		
		
12. House of Orange
	1. Conditions
		a. malloc(any) #A( writable, BOF to &top_chunk + 0xd8 )
		b. malloc(Size)
		c. malloc(smaller_than_0x50)
	2. Exploit 
		a. malloc(size) #A : top_chunk.size = (0x21000 - A.size) | 0x1
		b. top_chunk.size = ((0x21000 - A.size) & 0x00fff) | 0x1 : satisfy page aligned and set prev_inuse
		c. malloc(Size) : top_chunk.size < Size < mmp_mmap_threshhold, mmap triggered, new_top = old_top + page_size(0x21000), old_top freed but not consolidated with new_top because of old_top.size, UnsortedBin => old_top <bk---fd> main_arena+88 <bk---fd> old_top
		d. old_top.bk = io_list_all - 0x10 : io_list_all = old_top.fd + 0x9a8, UnsortedBin => io_list_all - 0x10 <bk---> old_top <bk---fd> main_arena+88 == Top[] <---fd> old_top
		e. old_top.prev_size = "/bin/sh\x00" : fp = "/bin/sh\x00"
		f. old_top.size = 0x61 : &(_chain) == base_address + 0x68 == Top[] + 0x68 == (0x60)SmallBin[4].bk, _chain = old_top
		g. *(old_top + 0xc0*SIZE_SZ) = 0 : old_top->_mode = 0, bypass "fp->_mode <= 0"
		h. *(old_top + 0x20*SIZE_SZ) = 2, *(old_top + 0x28*SIZE_SZ) = 3 : old_top->_IO_write_base = 2, old_top->_IO_write_ptr = 3, bypass "fp->_IO_write_ptr > fp->_IO_write_base"
		i. *(old_top + sizeof(_IO_FILE)*SIZE_SZ) = jump_table, jump_table[3] = target_function : sizeof(IO_FILE) == 0xd8, jump_table is writable memory (in general, old_top area), jump_table[3] is _IO_OVERFLOW
		j. malloc(smaller_than_0x50) : unlink old_chunk from UnsortedBin, UnsortedBin => <x> io_list_all - 0x10 <bk---fd> main_arena+88  <---fd> old_top, corrupted double-linked list Error, FSOP occurs, finally program calls target_function("/bin/sh")
	3. Value
		a. Execute arbitary function
		

13. unsafe unlink
	1. Conditions
		a. 2 malloc(larger_than_fast_chunk) #A( controlled by global pointer, writable, BOF to B.size ), #B 
		b. free(B) 
	2. Exploit
		a. global chunk_ptr = malloc(larger_than_fast_chunk) #A
		b. malloc(larger_than_fast_chunk) #B : B is victim chunk
		c. fake_chunk = A + 2*SIZE_SZ, fake_chunk.fd = &chunk_ptr - 3*SIZE_SZ, fake_chunk.bk = &chunk_ptr - 2*SIZE_SZ : &chunk_ptr - 2*SIZE_SZ <bk---fd> fake_chunk <bk---fd> &chunk_ptr - 3*SIZE_SZ. bypass "(P->fd->bk != P || P->bk->fd != P) == False"
		d. B.prev_size = A.size - 2*SIZE_SZ, B.size = B.size & 0xfff0 : B.prev_chunk == fake_chunk, B.size.prev_inuse = 1
		e. free(B) : unlink fake_chunk, chunk_ptr = &chunk_ptr - 3*SIZE_SZ, A and B are consolidated
		f. A[3] = target_address : chunk_ptr = target_address
	3. Value
		a. Manipulate arbitary memory data via global heap pointer 
		
		
14. large bin attack
	1. Conditions
		a.
	2. Exploit
		a. malloc(large_chunk_size) #A
		b. malloc(fast_chunk_size) : for preventing consolidating A with B
		c. malloc(large_chunk_size) #B
		d. malloc(fast_chunk) : for preventing consolidating B with C
		e. malloc(large_chunk_size) #C
		f. malloc(fast_chunk) : for preventing consolidating C with top chunk
		g. free(A), free(B) : UnsortedBin => B ---fd> A ---fd> main_arena+88
		h. malloc(smaller_than_A) : B is moved to large bin, parts of A is allocated and the remains of the A is into the unsorted bin. UnsortedBin => r_A ---fd> main_arena+88, (B.size)LargeBin => main_arena <bk---fd> B <bk---fd> main_arena
		i. free(C) : UnsortedBin => C ---fd> r_A ---fd> main_arena+88
		j. B.size = large_chunk_size_smaller_than_C, B.fd = 0, B.bk = target_address1 - 2*SIZE_Z, B.fd_nextsize = 0, B.bk_nextsize = target_address2 - 4*SIZE_SZ : LargeBin => target_address1 <bk---> B, target_address2.bk_nextsize <--- B.bk_nextsize
		k. malloc(smaller_than_C) : LargeBin  =>  C <bk---fd> (target_address1 - 2*SIZE_SZ) <bk--- B, C.bk_nextsize <--- target_address2 - 4*SIZE_SZ <--- B.bk_nextsize. In conclusion, *(target_address1) = C, *(target_address2) = C
	3. Value
		a. Bypass null check or Setting attack contidion related to overflow or heap
