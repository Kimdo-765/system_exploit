Heap Exploitation glibc version 2.25 it is based on how2heap + a

Have to list : Fix detail about main_arena + xx, Fix header_size(2*SIZE_SZ) to aligned(size)

0. heap
	1. Chunk
		a. Allocated chunk : prev_size | Size of chunk | Data | Size of next chunk
		b. Free chunk : Chunk | Chunk size | fd | bk | Unused Space | Size of next chunk ( main_arena <bk---fd> chunk1 <bk---fd> chunk2 <bk---fd> main_arena )
		c. Top chunk : the last in the memory, resized when malloc asks for more space from the os. If the requested size is smaller than 0x21000, extending the top chunk to adding page size, if not, mmap a new page
	2. Bin
		a. Fast bin : P.fd == next_chunk.prev_size, 0x10~0x58 bytes(0x20~0xb0 bytes in x64), 10 bins(single linked list), Free chunk Merge X
		b. Small bin : 0x10~0x1f8(0x20~0x3f0 in x64) bytes, 62 bins(double linked list), Free chunk Merge
		c. Large bin : 0x200~0x3f0(0x400~0x7e0 in x64) bytes, 63 bins(2 double linked list), Free chunk Merge, The fd_nextsize in the large bin chunk points to the first chunk in the list that is smaller than itself, and bk_nextsize points to the first chunk larger than itself.
		d. Unsorted bin : Freed Small, Large chunk, 1 bin(double linked list)
	3. malloc
		a. Memory aligned : chunk.size is always positioned at 0x~~~0 and chunk.prev_size is always positioned at 0x~~~8 (in x64), so if we request 0x~~0 bytes, 8 bytes are more allocated. So, in this document, aligned(chunk.size) = chunk.size - 0x8 or chunk.size - 0x10 ( 0x~8 or 0x~0 )
	4. free : FastChunkSize?FastBin:UnsrotedBin, FastBin,UnsortedBin=>SmallSize?SmallBin:LargeBin
		a. unlink : prev_inuse?nop:consolidate( P→fd->bk = P→bk; P→bk->fd = P->fd ), bin[x] => [a] <bk---fd> [b] <bk---fd> [c] in this situation consolidated [b+d] must move to bin[x+i], so do unlink() 

		
1. House of Force - Overwrite Top Chunk
	1. Conditions
		a.  1 malloc(size) #A( Writable, overflow to top_chunk.size )
		b.  1 malloc(evil_size) #B( We have to control size ) 
		c.  1 malloc(Size) #C( Writable )
	2. Exploit
		a. malloc(size) #A : set up memory
		b. *(A+real_A_size) = 0xffffffffffffffff : top_chunk.size = 0xffffffffffffffff, Now, malloc will never call mmap
		c. malloc(evil_size) #B : evil_size = target - old_top - 2*2*SIZE_SZ : Now, new_top = dest - 2*sizeof(long)
		e. malloc(Size) #C : new_top is allocated
	3. Value
		a. Manipulate data at semi-arbitary memory (higher address than top_chunk)


2. House of Spirit - Fake Chunk to Fast Bin
	1. Conditions
		a. 2 stack buffer : #A <---we can overwrite here, fast chunk size---> #B
		b. 1 malloc(ANY) : set up memory
		c. 1 free(A)
		d. 1 malloc(A to B - header_size)
	2. Exploit
		a. malloc(ANY) : set up memory
		b. fake_chunk1.size = size : fake_chunk1 + size == fake_chunk2
		c. fake_chunk2.size = Size : Size > 2*SIZE_SZ && Size < av->system_mem
		d. free(fake_chunk1 + 0x10) : FastBin => fake_chunk1
		e. malloc(fake_chunk1.size - 2*SIZE_SZ) : malloc at fake_chunk1
	3. Value
		a. Make buffer overflow attack vector
		
		
3. House of Lore - Bypass Small Bin Corrupted( bck->fd != victim )
	1. Conditions
		a. 2 stack buffer #fake_chunk1( writable, size >= 4*SIZE_SZ ), #fake_chunk2( writable, size >= SIZE_SZ )
		b. 3 malloc(small_bin_size) #A( writable after free ), #B, #C( writable )
		c. 1 malloc(large_bin_size)
		d. 1 free(A)
	2. Exploit
		a. malloc(small_chunk_size) #A
		b. fake_chunk1.prev_size = 0, fake_chunk1.size = 0, fake_chunk1.fd = A - 2*SIZE_SZ, fake_chunk1.bk = fake_chunk2, fake_chunk2.fd = fake_chunk1 : fake_chunk2 <bk---fd> fake_chunk1 <---fd> A
		c. malloc(large_chunk_size) : prevent consolidating the top chunk with A
		d. free(A) : UnsortedBin => A
		e. malloc(larger_than_A) : SmallBin => fake_chunk2 <bk---fd> fake_chunk1 <---fd> A <bk---fd> main_arena
		f. A.bk = fake_chunk1 : SmallBin => fake_chunk2 <bk---fd> fake_chunk1 <bk---fd> A <bk---fd> main_arena
		g. malloc(Size.A - 2*SIZE_SZ) #B : SmallBin => fake_chunk2 <bk---fd> fake_chunk1 <bk---fd> main_arena
		h. malloc(Size.A - 2*SIZE_SZ) #C : fake_chunk1 is allocated, SmallBin => fake_chunk2 <bk---fd> main_arena

	3. Value
		a. Make buffer overflow attack vector
		
		
4. first fit - UAF
	1. Conditions
		a. 2 malloc(size) #A( writable after free), #B( non-writable )
		b. 1 free(A) 
	2. Exploit
		a. malloc(size) #A
		a. free(A)
		b. malloc(size) #B : A == B, we can control B via A
	3. Value
		a. Manipulate heap data via duplicate heap pointer
		

5. fastbin dup - DFB
	1. Conditions
		a. 3 malloc(fast_chunk_size) #A, #B, #C 
		b. free(A), free(B), free(A)
		c. 3 malloc(fast_chunk_size)
	2. Exploit
		a. 3 malloc(fast_chunk_size) #A, #B, #C : C for preventing consolidating top chunk with B
		b. free(A) : FastBin => A ---fd> main_arena
		c. free(B) : FastBin => B ---fd> A ---fd> main_arena 
		d. free(A) : FastBin => A ---fd> B ---fd> A ---fd> B ...
		e. 3 malloc(A.size - 2*SIZE_SZ) : allocated A, B, A. A is duplicated
	3. Value
		a. Manipulate heap data via Duplicate heap pointer
		
		
6. fasetbin dup into stack
	1. Conditions
		a. 1 stack buffer #fake_chunk( size >= SIZE_SZ )
		b. 3 malloc(fast_chunk_size) #A, #B, #C
		c. free(A), free(B), free(A)
		d. 2 malloc(fast_chunk_size) #D( writable ), #E 
		e. 2 malloc(fast_chunk_size)
	2. Exploit
		a. 3 malloc(fast_chunk_size) #A, #B, #C : C for preventing consolidating top chunk with B
		b. free(A) -> free(B) -> free(A) : build up fastbin_dup
		c. 2 malloc(A.size - 2*SIZE_SZ) #D, #E : A and B is allocated, FastBin => A ---fd> B ---fd> A ...
		d. fake_chunk.size = A.size, D.fd = fake_chunk : FastBin A ---fd> fake_chunk ---x>
		e. 2 malloc(A.size -2*SIZE_SZ) : A and fake_chunk is allocated
	3. Value
		a. Make buffer overflow attack vector


7. House of einherjar - off by one
	1. Conditions
		a. 1 buffer #fake_chunk( size >= 6*SIZE_SZ )
		b. 2 malloc(size) #A( writable, off by one to B.size.prev_inuse ), #B
		c. free(B)
		d. malloc(Size)
	2. Exploit
		a. 2 malloc(size) #A, #B
		b. fake_chunk.prev_size = ANY, fake_chunk.size = ANY, fake_chunk.fd = fake_chunk, fake_chunk.bk = fake_chunk, fake_chunk.fd_nextsize = fake_chunk, fake_chunk.bk_nextsize = fake_chunk
		c. *(A+real_A_size - SIZE_SZ) = (B - 2*SIZE_SZ) - fake_chunk, *(A+real_A_size) = 0 : B.prev_size = -(B to fake_chunk), B.size = B.size | 0xff00, via off by one
		d. fake_chunk.size = B.prev_size : bypass "unlink : corrupted size vs. prev_size"
		e. free(B) : 
			1. If B is top chunk( malloc(C) before free(B) ) : fake_chunk & B & top_chunk are consolidated, unlink fake_chunk, new_top == fake_chunk
				a. malloc(Size) : fake_chunk is allocated
			2. If B is not top chunk : fake_chunk & B are consolidated, unlink fake_chunk, UnsortedBin => fake_chunk
				a. fake_chunk.size = SIZE, 2*SIZE_SZ <= SIZE <= av->system_mem : bypass " malloc() : memory corruption"
				b. malloc(smaller_than_fake_chunk_size) : fake_chunk is allocated
	3. Value
		a. Make buffer overflow attack vector
		
		
8. Poison null byte - off by one
	1. Conditions
		a. 4 malloc(larger_than_fast_chunk) #A( writable, off by one to B.size ), #B( writable ), #C, #D
		b. free
	2. Exploit
		a. 4 malloc(larger_than_fast_chunk) #A, #B, #C, #D : D for preventing consolidating top chunk with C
		b. free(B) : UnsortedBin => B, C.prev_size = B.size, C.size.prev_inuse = 0
		d. *(A+real_A_size) = 0 : B.size = B.size|0xff00
		c. *(B + B.size|0xff00 - 2*SIZE_SZ) = B.size|0xff00 : fake_C_prevsize = B.size|0xff00, fake_C_prevsize == B.size, bypass "unlink : corrupted size vs. prev_size"
		e. malloc(smaller_than_B) #B1 : C.prev_size is not changed, B is allocated, unlink B, (B.size - B1.size)UnsortedBin => remain_B
		f. malloc(smaller_than_B1) #B2 : C.prev_size is not changed, B+B1.size is allocated, unlink remain_B, (B.size - B1.size - B2.size)UnsortedBin => remain2_B
		g. free(B1) : (B1.size)UnsortedBin => B2
		h. free(C) : C.prev_chunk == C - C.prev_size == B1, B1 and C are consolidated, unlink B1, (C.prev_size + C.size)UnsortedBin => B1
		i. malloc(C.prev_size + C.size - 2*SIZE_SZ) : B1 is allocated, overlapping B2
	3. Value
		a. Manipulate heap data via overlapping


9. Overlapping chunks
	1. Conditions
		a.  3 malloc(larger_than_fast_chunk) #A( writable, BOF to B.size ), #B, #C 
		b.  free(B) 
		c.  malloc(B.size + C.size - SIZE_SZ) 
	2. Exploit 
		a. 3 malloc(larger_than_fast_chunk) #A, #B, #C : C for preventing consolidating top chunk with B
		b. free(B) : UnsortedBin => B
		c. B.size = (B.size + C.size) | 0x1
		d. malloc(B.size + C.size - SIZE_SZ) : B + C is allocated
	3. Value
		a. Manipulate heap data via overlapping


10. Overlapping chunks 2
	1. Conditions
		a. [ 5 malloc() #A( writable, BOF to B.size ), #B, #C, #D, #E ]
		b. [ free(D) ]
		c. [ malloc(Size) ]
	2. Exploit
		a. 5 malloc(larget_than_fast_chunk) #A, #B, #C, #D, #E : E for preventing consolidating top chunk with D
		b. free(D) : UnsortedBin => D
		c. B.size = (B.size + C.size) | 0x1 : B.next_chunk => D
		d. free(B) : D.prev_size = B.size + C.size, B & D are consolidated, unlink B1, UnsortedBin => B
		e. malloc(B.size + C.size + D.size) : B + C + D is allocated
	3. Value
		a. Manipulate heap data via overlapping
		
		
11. unsorted bin attack
	1. Conditions
		a. 2 malloc(larger_than_fast_chunk) #A( writable after free ), #B 
		b. free(A) 
		c. malloc(aligned(A.size))
	2. Exploit 
		a. 2 malooc(larger_than_fast_chunk) #A, #B : B for preventing consolidating top chunk with A
		b. free(A) : UnsortedBin => main_arena <bk---fd> A <bk---fd> main_arena
		c. A.bk = target_address - 2*SIZE_SZ : target_address <bk---> A <bk---fd> main_arena
		d. malloc(A.size - 2*SIZE_SZ) : unlink A, target_address <bk---fd> main_arena, *(target_address) = main_arena
	3. Value
		a. Bypass null check or Setting attack contidion related with main_arena
		
		
12. House of Orange
	1. Conditions
		a. malloc(any) #A( writable, BOF to &top_chunk + 0xd8 )
		b. malloc(Size)
		c. malloc(smaller_than_0x50)
	2. Exploit 
		a. malloc(size) : top_chunk.size = (0x21000 - aligned(size)) | 0x1
		b. top_chunk.size = aligned((0x21000 - aligned(size)) & 0x00fff) | 0x1 : satisfy page aligned and set prev_inuse
		c. malloc(Size) : top_chunk.size < Size < mmp_mmap_threshhold, mmap triggered, new_top = old_top + page_size(0x21000), old_top freed but not consolidated with new_top because of old_top.size, UnsortedBin => old_top <bk---fd> main_arena+88 <bk---fd> old_top
		d. old_top.bk = io_list_all - 0x10 : io_list_all = old_top.fd + 0x9a8, UnsortedBin => io_list_all - 0x10 <bk---> old_top <bk---fd> main_arena+88 == Top[] <---fd> old_top
		e. old_top.prev_size = "/bin/sh\x00" : fp = "/bin/sh\x00"
		f. old_top.size = 0x61 : &(_chain) == base_address + 0x68 == Top[] + 0x68 == (0x60)SmallBin[4].bk, _chain = old_top
		g. *(old_top + 0xc0*SIZE_SZ) = 0 : old_top->_mode = 0, bypass "fp->_mode <= 0"
		h. *(old_top + 0x20*SIZE_SZ) = 2, *(old_top + 0x28*SIZE_SZ) = 3 : old_top->_IO_write_base = 2, old_top->_IO_write_ptr = 3, bypass "fp->_IO_write_ptr > fp->_IO_write_base"
		i. *(old_top + sizeof(_IO_FILE)*SIZE_SZ) = jump_table, jump_table[3] = target_function : sizeof(IO_FILE) == 0xd8, jump_table is writable memory (in general, old_top area), jump_table[3] is _IO_OVERFLOW
		j. malloc(smaller_than_0x50) : unlink old_chunk from UnsortedBin, UnsortedBin => <x> io_list_all - 0x10 <bk---fd> main_arena+88  <---fd> old_top, corrupted double-linked list Error, FSOP occurs, finally program calls target_function("/bin/sh")
	3. Value
		a. Execute arbitary function
		

13. unsafe unlink
	1. Conditions
		a. [ 2 malloc[0x80] #A( writable, BOF to B.size ), #B ]
		b. [ free(B) ]
		c. [ global pointer buf1( writable through pointer ) ]
	2. Exploit
		a. 2 malloc(0x80) #A, #B
		b. global buf1 = &A 
		c. Create Fake Chunk, (&A + 0x10) = 0x0, (&A + 0x18) = 0x0, (&A + 0x20) = &buf1 - 0x18, (&A + 0x28) = &buf - 0x10 : P.next_chunk = P, P->fd->bk == P, P->bk->fd == P
		d. B.prev_size = 0x80, B.size = 0x90 (delete prev_inuse)
		e. free(B) : consolidate works, buf1 = fake_chunk.fd(&buf1 - 0x18)
		f. Overwrite buf1 : *buf1 = "A"*0x18 + target_address, e.g) GOT
		g. Overwrite target_address : *buf1 = something, e.g) &system
	3. Value
		a. Manipulate ANY memory that is writable to arbitary data
		
		
14. large bin attack
	1. Conditions
		a.
	2. Exploit
		a. malloc(large_chunk) #A
		b. malloc(fast_chunk) : prevent consolidating #A with #B
		c. malloc(large_chunk.larger_than_A) #B
		d. malloc(fast_chunk) : prevent consolidating #B with #C
		e. malloc(B.size - header_size) #C
		f. malloc(fast_chunk) : prevent consolidating #C with top chunk
		g. free(A), free(B) : unsorted bin [ B <---> A ]
		h. malloc(size <= A.size) #D : B is moved to large bin freelist, parts of A is allocated and the remaining of the freed firtst large chunk into the unsorted bin[ &A + d.size + header_size ], large bin[ B ]
		i. free(C) : unsorted bin [ C <---> &A + D.size + header_size ], large bin[ B ]
		j. B.size = large_chunk.smaller_than_A, B.fd = 0, B.bk = target_address1 - 0x10, B.fd_nextsize = 0, B.bk_nextsize = target_address2 - 0x20
		k. malloc(D.size) : large bin [ target_address1 - 0x10 <bk--- B ] => [ C <bk--- target_address - 0x10 <bk--- B ], target_address1 = &C, target_address2 = &C
	3. Value
		a. Manipulate ANY momory that is writable to &C
