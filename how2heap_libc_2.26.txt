Heap Exploitation glibc version 2.26 based on how2heap

0. heap
	1. Chunk 
		a. Allocated chunk : [prev_size or Chunk] | Size of chunk | Data | Size of next chunk
		b. Free chunk : [Chunk or prev_size] | Chunk size | [fd] | bk | [fd_nextsize] | [bk_nextsize] | Unused Space | Size of next chunk ( main_arena <bk---fd> chunk1 <bk---fd> chunk2 <bk---fd> main_arena )
		c. Top chunk : the last in the memory, resized when malloc asks for more space from the os. If the requested size is smaller than 0x21000, extending the top chunk to adding page size, if not, mmap a new page
	2. Bin
		a. Fast bin : P.fd == next_chunk.prev_size, 0x20~64*SIZE_SZ/4(in default), 10 bins(single linked list, LIFO), Free chunk Merge X
		b. Small bin : 0x10~0x1f8(0x20~0x3f0 in x64) bytes, 62 bins(double linked list, FIFO), Free chunk Merge
		c. Large bin : 0x200~0x3f0(0x400~0x7e0 in x64) bytes, 63 bins(2 double linked list), Free chunk Merge, The fd_nextsize in the large bin chunk points to the first chunk in the list that is smaller than itself, and bk_nextsize points to the first chunk larger than itself.
		d. Unsorted bin : Freed Small, Large chunk, 1 bin(double linked list)
	3. malloc
		a. Memory aligned : chunk.size is always positioned at 0x~~~8 and chunk.prev_size is always positioned at 0x~~~0 (in x64), so if we request 0x~~0 bytes, 8 bytes are more allocated for prev_size for prev_size, chunk.size = requested(chunk) + 0x8(chunk.size) + 0x8, usable(chunk) = requested(chunk) + 0x8
	4. free : FastChunkSize?FastBin:UnsrotedBin, FastBin,UnsortedBin=>SmallSize?SmallBin:LargeBin
		a. unlink : prev_inuse?nop:consolidate( P→fd->bk = P→bk; P→bk->fd = P->fd ), bin[x] => [a] <bk---fd> [b] <bk---fd> [c] in this situation consolidated [b+d] must move to bin[x+i], so do unlink() 


1. tcache dup
	1. Conditions
		a. malloc(size) #A 
		b. 2 free(A)
		c. malloc(size)
	2. Exploit
		a. malloc(size) #A
		b. free(A), free(A) : tcache_entry -> A -> A -> A ...
		c. malloc(size), malloc(size), ... #A, #A, ...
	3. Value
		a. Duplicate heap control


2. tcache poisoning
	1. Conditions
		a. 3 malloc(size) #A( writable after free to fd ), #B, #C
		b. free(A)
	2. Exploit 
		a. malloc(size) #A
		b. free(A) : tcache => &A.fd
		c. A.fd = target_address : tcache => &A.fd ---> target_address
		d. 2 malloc(size) #B, #C : B == A, &C == target_address
	3. Value
		a. Manipulate ANY memory to arbitary data
	

3. tcache house of spirit
	1. Conditions
		a. writable fake_chunk( size is least 2*SIZE_SZ )
		b. 2 malloc() #A, #B( writable )
		c. free(&fake_chunk.fd)
	2. Exploit
		a. malloc(any) : set up heap memory
		b. fake_chunk.size = size + header_size
		c. free(&fake_chunk.fd) : &fake_chunk.fd must be 16 byte aligned 
		d. malloc(size) : fake_chunk allocated
	3. Value
		a. Overflow small writable memory
		

4. house of botcake
	1. Conditions
		a. 7 malloc(size) #A
		b. 2 malloc(size) #B, #C
		c. malloc(any_size)
		d. free() is free!
		e. C can be double freed
		f. malloc(large_size) #D( writable )
	2. Exploit
		a. 7 malloc(size) #A : for freeing and fill up tcache list later
		b. malloc(size) #B : for later consolidation
		c. malloc(size) #C : victim chunk
		d. malloc(any_size) : for preventing consolidation
		e. 7 free(#A) : S1. fill up tcache list
		f. free(C) : unsorted bin => C
		g. free(B) : B consolidate with the C
		h. malloc(size) : taking one out from tcache list
		i. free(C) : add C to tcache list *Double Free ocuurs*, tcache => C.fd ---> dum6.fd ---> dum5.fd ...
		j. malloc(large_size) #D 
		k. *(&D + size + header_size) = target_address : we can overwrite C.fd to target_address,  tcache => C.fd ---> target_address

		l. malloc(size) : C is poped from tcache list
		m. malloc(size) : target_addr is poped from poisoned tcache list
		n. free(C), overwrite C.fd, 2 malloc(size), repeat 
	3. Value
		a. Manipulate ANY memory to arbitary data in MULTI time


*----------tcache option disabled----------*


5. large bin attack
	1. Conditions
		a. 5 malloc(large_chunk_size) #A, #B( writable to bk_nextsize ), #C, #D, #E
		b. 3 malloc(fast_chunk_size)
		c. free(A), free(B), free(C)
	2. Exploit
		a. malloc(large_chunk_size) #A
		b. malloc(fast_chunk_size) : for prevent consolidating A and B
		c. malloc(large_chunk_size+a) #B
		d. malloc(fast_chunk_size) : for prevent consolidating B and C
		e. malloc(large_chunk_size+a) #C
		f. malloc(fast_chunk_size) : for prevent consolidating C and top chunk
		g. free(A), free(B) : unsorted bin => B <bk---fd> A
		h. malloc(smaller_than_A) #D : unsorted bin => A+D.size, large bin => B
		i. free(C) : unsorted bin => C <bk---fd> A + D.size
		j. B.size = smaller_than_B, B.fd = 0, B.fd_nextsize = 0, B.bk = target_address - 0x10, B.bk_nextsize = target_address - 0x20 : large bin => target_address - 0x10 <bk---> B, target_address - 0x20 <bk_nextsize---> B
		k. malloc(smaller_than_C) #E : item inserted at double linked list! B.bk.fd = C, B.bk_nextsize.fd_nextsize = C
	3. Value
		a. Manipulate ANY memory to large unsinged long value


6. overlapping chunks
	1. Conditions
		a. 3 malloc(size) #A, #B, #C
		b. free(B)
		c. modify B.size through dup or overflow
		d. malloc(B.size + C.size + 0x1)
	2. Exploit
		a. 3 malloc(size) #A, #B, #C
		b. free(B) : unsorted bin => B
		c. B.size = B.size + C.size + 0x1(prev_inuse)
		d. malloc(modified_B.size - header_size) #D : D == B, C is overlapped by D
	3. Value
		a. Heap memory overflow


7. unsafe unlink 2.0
	1. Conditions
		a. global *ptr = malloc(larger(fast_chunk)) #A( writable, overflow to B.size.prev_inuse ) 
		b. malloc(fast_chunk_size) #B
		c. free(B)
	2. Exploit
		a. global ptr = malloc(larger(fast_chunk)) #A
		b. malloc(larger(fast_chunk)) #B
		c. create fake chunk inside A : fake_chunk.fd = &ptr - 0x18, fake_chunk.bk = &ptr - 0x10
		d. B.prev_size = A.size - 2*SIZE_SZ, B.size.prev_inuse = 0
		e. free(B) : consolidating and unlink! fake_chunk.fd.bk(== ptr) = fake_chunk.bk(== &ptr - 0x10), fake_chunk.bk.fd(== ptr) = fake_chunk.fd(== &ptr - 0x18)
		f. overwrite ptr through ptr!!! : memset(ptr, "A"*0x18 + target_address, 0x20), ptr = target_address
		g. write target_address through ptr!!!
	3. Value
		a. Manipulate ANY memory to arbitary data


8. unsorted bin attack
	1. Conditions
		a. 2 malloc(size) #A( writable ), #B
		b. free(A)
		c. malloc(size)
	2. Exploit
		a. malloc(larger(fast_chunk)) #A
		b. malloc(size) : prevent to consolidate the top chunk with A
		c. free(A) : unsorted bin => main_arena <bk---fd> A <bk---fd> main_arena
		d. A.bk = target_adress - 0x10 : unsorted bin => target_address - 0x10 <bk---> A <bk---fd> main_arena
		e. malloc(requested(A)) : A.bk.fd(==target_address) = A.fd(==main_arena)
	3. Value
		a. Bypass "if" or rewriteing the global_max_fast in libc for further fastbin attack


9. unsorted bin into stack
	1. Conditions
		a. 2 malloc() #A( larger(fast_chunk), writable )
		b. free(A)
		c. malloc(size)
	2. Exploit
		a. malloc(larger(fast_chunk)) #A
		b. malloc(any) : prevent to consolidate the top chunk with A
		c. free(A) : unsorted bin => A
		d. stack.size = smaller(A), stack.bk = &(stack.prev_size) : create fake chunk on the stack
		e. A.bk = &(stack.prev_size) : unsorted bin => stack <bk---> stack <bk---> A
		f. malloc(requested(stack)) : size detecting... A.size != size, stack.size == size !!! allocate stack
	3. Value
		a. Make memory overflow attack vector


10. house of einherjar
	1. Conditions
		a. 3 malloc() #A( writable, overwrite to B.prevsize )
		b. fake_chunk
		c. free(B)
	2. Exploit
		a. malloc(size) #A
		b. fake_chunk.prev_size = Size, fake_chunk.size = Size, fake_chunk.fd = chunk, fake_chunk.bk = fake_chunk, fake_chunk.fd_nextsize = fake_chunk, fake_chunk.bk_nextsize = fake_chunk : Create fake chunk
		c. malloc(Size) #B 
		d. A = "A" * B.size : B.size.prev_inuse = 0, you can use off by one to delete prev_inuse bit if B.size's last two bytes are 00
		e. B.prev_size = &B - header_size - &fake_chunk, chunk.size = B.prev_size
		f. free(B) : consolidate with fake chunk, chunk.size == B.size + chunk.prev_size
			1. if we malloc before f., set chunk.size = B.size after free(B)
			2. unsorted bin => chunk
		g. malloc(any_size) : fake_chunk address is allocated
			1. malloc(B.size) : fake_chunk address is allocated
	3. Value
		a. Make memory overflow attack vector


11. house of lore
	1. Conditions
		a. 3 malloc(small_chunk) #A( writable )
		b. 2 fake_chunk
		3. free(A)
	2. Exploit
		a. malloc(small_chunk) #A
		b. fake_chunk1.prev_size = 0, fake_chunk1.size = 0, fake_chunk1.fd = A : Create fake chunk on the stack, bypass the check of small bin corrupted
		c. fake_chunk1.bk = fake_chunk2, fake_chunk2.fd = fake_chunk1 : bypass the check of small bin corrupted, fake_chunk2 <bk---fd> fake_chunk1 <---fd> A
		d. malloc(large_chunk) : avoid consolidating the top chunk with the A
		e. free(A) : unsorted bin => A
		f. malloc(larger(A)) : small bin => main_arena <bk---fd> A <bk---fd> main_arena
		g. A.bk = fake_chunk1 : small bin => fake_chunk2 <bk---fd> fake_chunk1 <bk---fd> A <bk---fd> main_arena
		h. malloc(requested(A)) : small bin => fake_chunk2 <bk---fd> fake_chunk1 <bk---fd> main_arena
		i. malloc(requested(A)) : small bin => fake_chunk2 <bk---fd> main_arena, you can control fake_chunk1
	3. Value
		a. Make memory overflow attack vector
